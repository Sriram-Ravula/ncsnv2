training:
  batch_size: 16
  n_epochs: 30000  #number of training epochs - superceded by n_iters 
  n_iters: 30001  #maximum number of training iterations - stops before n_epochs if needed
  snapshot_freq: 1000 #frequency with which to save weights and sample from output
  snapshot_sampling: true 
  anneal_power: 2 
  log_all_sigmas: false

sampling:
  batch_size: 36  #number of samples to produce for sampling
  data_init: false
  step_lr: 0.0000016  #epsilon - step size for sampling during Langevin Dynamics
  n_steps_each: 10 #T - number of inner loop steps
  ckpt_id: 80000  
  final_only: true
  fid: false
  denoise: true #whether to add \sigma^2 * s_theta(x_T, \sigma_T) to the final output
  num_samples4fid: 10000
  inpainting: false
  interpolation: false
  n_interpolations: 8

#unused
fast_fid:
  batch_size: 1000
  num_samples: 1000
  step_lr: 0.0000009
  n_steps_each: 3
  begin_ckpt: 100000
  end_ckpt: 80000
  verbose: false
  ensemble: false

test:
  begin_ckpt: 10
  end_ckpt: 100
  batch_size: 100

data:
  dataset: "RTM_N"
  image_size: 256
  channels: 1
  logit_transform: false
  uniform_dequantization: false
  gaussian_dequantization: false
  random_flip: false
  rescaled: false
  num_workers: 8

model:
  ema: true #use an exponential moving avergae?
  ema_rate: 0.999 #m, for parameters: \theta^' = m*\theta^' + (1 - m)\theta_i
  spec_norm: false
  sigma_dist: rtm #the progression of the sigma values
  num_classes: 33
  n_shots: [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,24,28,32,36,40,50,60,70,80,90,100,120,140,160,180,200,240]
  lambdas_list: [5, 4.11743917, 3.39066106, 2.79216813, 2.29931649, 1.89345916, 1.55924058, 1.28401565, 1.05737126, 0.87073237, 0.71703751, 0.59047167, 0.48624624, 0.40041786, 0.32973924, 0.27153625, 0.2236068 , 0.18413748, 0.15163497, 0.12486956, 0.10282856, 0.08467807, 0.06973136, 0.05742293, 0.04728708, 0.03894034, 0.03206689, 0.0264067 , 0.02174559, 0.01790723, 0.01474639, 0.01214347, 0.01]
  normalization: InstanceNorm++
  nonlinearity: elu
  ngf: 128

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.0001
  beta1: 0.9
  amsgrad: false
  eps: 0.00000001
