training:
  batch_size: 1
  n_epochs: 1000  #max number of training epochs
  checkpoint_freq: 25 #save weights every n epochs
  test_freq: 25 #test every n epochs
  sample_freq: 25 #sample every n epochs
  anneal_power: 2 

sampling:
  batch_size: 8  #number of samples to produce for sampling
  step_lr: 0.0000016  #epsilon - step size for sampling during Langevin Dynamics
  n_steps_each: 100 #T - number of inner loop steps
  final_only: true
  denoise: false #whether to add \sigma^2 * s_theta(x_T, \sigma_T) to the final output
  add_noise: false #whether to add stochastic noise during Langevin dynamics (false means MAP estimation)
  shot_idx: 0 #the index of the n_shots to use when initializing samples 


data:
  dataset: "IBALT_RTM_N"
  image_size: [1024,256]
  channels: 1
  random_flip: true
  num_workers: 8
  logit_transform: false
  uniform_dequantization: false
  gaussian_dequantization: false
  rescaled: true

model:
  ema: true #use an exponential moving avergae?
  ema_rate: 0.999 #m, for parameters: \theta^' = m*\theta^' + (1 - m)\theta_i
  spec_norm: false
  sigma_dist: rtm_dynamic #the progression of the sigma values
  num_classes: 20
  n_shots: [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 110, 120, 130, 140, 150, 160, 170, 180]
  lambdas_list: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
  normalization: InstanceNorm++
  nonlinearity: elu
  ngf: 64

optim:
  weight_decay: 0.000
  optimizer: "Adam"
  lr: 0.00002
  beta1: 0.9
  amsgrad: false
  eps: 0.001
